# 空心六边形结构优化 - 基于深度强化学习

本项目使用深度强化学习（Deep Q-Network, DQN）来优化空心六边形结构，目标是在满足受力条件的前提下最小化结构面积。

## 项目背景

在工程结构设计中，我们常常需要在满足强度要求的同时，尽可能减少材料用量（即结构面积或体积）。这是一个典型的多目标优化问题。本项目以空心六边形结构为例，演示如何使用深度强化学习来解决此类问题。

## 项目结构

- `hexagon_env.py`: 定义空心六边形优化环境，包括状态空间、动作空间和奖励函数
- `dqn_model.py`: 实现DQN算法，包括神经网络模型、经验回放和训练逻辑
- `train_hexagon_dqn.py`: 主训练脚本，用于训练DQN智能体并可视化结果
- `results/`: 存储训练结果、模型和可视化图像的目录

## 强化学习原理

### 基本概念

强化学习是机器学习的一个分支，它关注如何使智能体在环境中通过试错学习来最大化累积奖励。强化学习的核心元素包括：

1. **智能体(Agent)**: 学习做决策的实体，在本项目中是DQN智能体
2. **环境(Environment)**: 智能体交互的外部系统，在本项目中是空心六边形结构
3. **状态(State)**: 环境的当前情况，在本项目中是内外六边形边长和应力状态
4. **动作(Action)**: 智能体可以执行的操作，在本项目中是调整内外六边形边长
5. **奖励(Reward)**: 环境对智能体行为的反馈，在本项目中是负的结构面积（面积越小奖励越大）
6. **策略(Policy)**: 智能体的行为策略，决定在给定状态下应该采取什么动作

### 马尔可夫决策过程(MDP)

强化学习问题通常被建模为马尔可夫决策过程，包含以下要素：
- 状态集合S
- 动作集合A
- 状态转移概率P(s'|s,a)
- 奖励函数R(s,a,s')
- 折扣因子γ∈[0,1]，用于平衡即时奖励和未来奖励

### 价值函数

强化学习中有两个重要的价值函数：

1. **状态价值函数V(s)**: 从状态s开始，遵循当前策略能获得的期望累积奖励
2. **动作价值函数Q(s,a)**: 在状态s下执行动作a，然后遵循当前策略能获得的期望累积奖励

### 深度Q网络(DQN)

传统的Q学习在状态空间较大时面临维度灾难问题。DQN通过深度神经网络来近似Q函数，解决了这一问题。DQN的关键创新包括：

1. **经验回放(Experience Replay)**: 存储智能体的经验(s,a,r,s')，随机采样打破样本相关性
2. **目标网络(Target Network)**: 使用单独的网络计算目标Q值，提高训练稳定性
3. **ε-贪婪策略(ε-greedy Policy)**: 平衡探索与利用，以ε的概率随机探索，以1-ε的概率选择当前最优动作

## 本项目中的强化学习实现

### 环境设计 (hexagon_env.py)

1. **状态空间**: 三维连续空间 [内边长, 外边长, 应力状态]
2. **动作空间**: 四个离散动作 [减小内边长, 增大内边长, 减小外边长, 增大外边长]
3. **奖励设计**: 
   - 有效结构: 负的空心六边形面积（面积越小奖励越大）
   - 无效结构: 大的负奖励(-1000)作为惩罚
4. **终止条件**: 达到最大步数或结构无效

### DQN实现 (dqn_model.py)

1. **网络结构**: 三层全连接神经网络(状态维度→64→64→动作维度)
2. **经验回放**: 使用双端队列存储和采样经验(状态,动作,奖励,下一状态,是否结束)
3. **双网络架构**: 
   - 主网络(model): 用于选择动作和更新
   - 目标网络(target_model): 用于计算目标Q值
4. **ε-贪婪策略**: 
   - 初始ε=1.0，随训练进行衰减
   - 以ε的概率随机选择动作，以1-ε的概率选择Q值最大的动作
5. **学习过程**: 
   - 从经验回放缓冲区采样批量经验
   - 计算当前Q值: Q(s,a)
   - 计算目标Q值: r + γ * max_a' Q'(s',a')
   - 使用均方误差损失更新网络参数

### 训练过程 (train_hexagon_dqn.py)

1. **初始化**: 创建环境和DQN智能体
2. **训练循环**: 
   - 每个episode重置环境
   - 智能体与环境交互，收集经验
   - 定期从经验回放缓冲区中学习
   - 定期更新目标网络
3. **结果记录**: 
   - 保存训练过程中的奖励曲线
   - 保存最佳模型和结构参数
   - 可视化最佳结构

## 运行方法

1. 确保已安装所需依赖：
   ```
   pip install numpy matplotlib torch gym
   ```

2. 运行训练脚本：
   ```
   python train_hexagon_dqn.py
   ```

3. 训练完成后，结果将保存在`results`目录中，包括：
   - 训练过程中的奖励曲线
   - 最佳结构参数和可视化图像
   - 训练过程中的中间结果

## 结果分析

训练完成后，可以观察以下指标：

1. **奖励曲线**: 反映训练过程中智能体性能的提升
2. **最佳结构**: 满足应力约束条件下面积最小的结构
3. **探索率变化**: 反映智能体从探索到利用的转变过程

## 扩展与改进

1. **环境模型优化**: 使用更精确的有限元分析代替简化的应力计算模型
2. **算法改进**: 尝试其他强化学习算法如DDPG、PPO等
3. **多目标优化**: 同时考虑结构面积、重量、成本等多个优化目标
4. **约束处理**: 改进对结构约束的处理方式，如使用拉格朗日乘子法

## 参考资料

1. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.
2. Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
3. Lillicrap, T. P., et al. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.